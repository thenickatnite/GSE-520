---
title: 'GSE 520: Problem Set #18'
author: 
  - "Nick Brown"
  - "Nathaniel Cinnamon"
  - "Ian Donovan"
output: html_document
---

Governments of countries with low or declining birth rates will often offer their citizens an incentive to have children. This was the case in Spain, so in 2007 the Spanish government implemented a universal child benefit. This benefit gave new mothers a one-time payment of around $\$3,900$ almost immediately after child birth. The universal child benefit went into effect in July of 2007 and after it was announced, any mother who had given birth on or after July 1, 2007 was eligible to receive the benefit. In "The Effect of a Universal Child Benefit on Conceptions, Abortions, and Early Maternal Labor Supply", author Libertad Gonzalez uses a regression discontinuity design to see if the number of conceptions (among other things, you'll be replicating the conception estimates for this problem set) increased following the implementation of the benefit. You can find the data set for this paper in data file titled: ```births.RData```.

The data set consists of estimated conceptions per month from 2000 to 2009, this is represented as the variable $n$ in the data set. The variable $log\_n$ is the log number of estimated conceptions per month. The month of conception variable $mc$, has been centered using the July 2007 policy implementation date (so for July of 2007 $mc=0$, August of 2007 $mc=1$, June of 2007 $mc=-1$, etc.). There are also $mc2$ and $mc3$ variables which represent the quadratic and cubic trends of the linear $mc$ variable. Another variable is the $days$ variable which represents the number of days in the month of conception. The variable of interest in the paper is the $post$ variable, a binary variable which is $1$ if the $mc$ is after July 2007 and $0$ if the $mc$ is before July 2007.

### Question 1

(a) Explain what aspects of this econometric question makes it suitable for using regression discontinuity (RD), i.e., what important element(s) must be observed in your data for your RD to work?

>**Solution:**
In order for a regression discontinuity to work, you must have some sort of running variable and a cutoff point. Once the running variable passes the cutoff point there must be some sort of treatment applied. An example of this is the minimum legal drinking age. In this example, the running variable is an individual's age and the cutoff is the drinking age of 21. After an individual turns 21, the treatment applied is legal access to alcohol.

(b) Suppose a researcher specifies a very simple regression of the form:
$$Y_i = \alpha + \rho D_b + \gamma b + \varepsilon_b$$
The variable $D_b$ is a binary variable which is 1 when the running variable $b$ passes some cutoff figure and 0 otherwise. Why do we need to $b$ the running variable itself as a control variable?

>**Solution:**
We need $b$ to eliminate any omitted variable bias in the problem. $D_b$ is exclusively determined by the the running variable $b$, which means if we exclude the running variable from the regression equation, we would have introduced omitted variable bias into the equation.


### Question 2  

(a) Let's start by getting summary statistics for monthly conceptions.  For this analysis and the analysis below, subset the data from $mc=-90$ to $mc = 29$. Your subset should consist of 120 observations.  What is the average number of conceptions per month?  What is the correlation between $mc$ and monthly conceptions pre-July 2007?  What does this suggest about the trend in conceptions over time before the policy?

>**Solution:**  
>```{R}
load('births.RData')
suppressMessages(library("AER"))
subset1 = n[n$mc > -91 & n$mc < 30, ]
summary(subset1)
cor(n$n[n$mc < 0], n$mc[n$mc < 0])
```
>The average number of conceptions per month is 38021 (number of estimated conceptions per month is the variable $n$). The correlation between $mc$ and monthly conceptions before July 2007 is $0.718$. A positive correlation between the monthly conceptions and $mc$ suggests conceptions were increasing over time.

(b) Estimate a regression of the form: 
$$log\_n = \beta_0 + \beta_1 post + \beta_2 mc + \beta_3 mc2 + \beta_4 mc3 + \beta_5 days$$
Print the summary of the coefficients, t-scores, and p-values (make sure you use robust standard errors) from the regression you ran. Interpret the coefficient for the estimate of $\beta_1$. What effect does the universal child benefit appear to have on conceptions in Spain? Does this effect make economic sense?

>**Solution:**  
>```{R}
mod1 = lm(log_n ~ factor(post) + mc + mc2 + mc3 + days, data = subset1)
round(coeftest(mod1, vcov. = vcovHC(mod1, "HC1")), 5)
```
>The coefficient for $\beta_1$ is $0.049$ and significant at the $1\%$ significance level. This means when $post$ is 1 (so after the Universal Child Benefit went into effect), there is an $0.049\%$ increase in conceptions per month. This suggests the child benefit was successful in increasing conceptions. This does make economic sense, as the benefit would be an added incentive for having a child (aside for the personal reasons).

(c) In the regression you ran above you should fail to reject the null hypothesis that the coefficient on $mc$=0 at the 5\% level.  State as best you can what it means to reject this null hypothesis.  Does this variable have any economic importance at all? 

>**Solution:**  
To reject the null hypothesis here would mean that the month of conception has a significant impact on the number of children born in a particular month.

(d) Let's graph the data and regression lines and see if a clear discontinuity shows itself in the graph. The lines generated will be done using the loess smoother, both for aesthetic and ease of code reasons. For this question you will need to download and attach the ```ggplot2``` package in R. Here is an example of the code you can use to plot the data: 
    ```
ggplot(first_subset (this should be whatever the name of your subset is), aes(x = mc, y = log_n, colour = factor(post))) + 
  geom_point() + 
  xlab("Month of Conception Before/After July 2007") + 
  ylab("Log Number of Conceptions") + 
  ggtitle("Estimated Number of Log Conceptions Per Month") + 
  stat_smooth(method = loess)
```
    Plot the graph and then interpret what you see. Is there a clear discontinuity following the policy implementation (is there a jump in estimated conceptions)? What are the implications if a clear discontinuity does not appear graphically?

>**Solution:** 
>```{R}
suppressMessages(library("ggplot2"))
ggplot(subset1, aes(x = mc, y = log_n, colour = factor(post))) + 
  geom_point() + 
  xlab("Month of Conception Before/After July 2007") + 
  ylab("Log Number of Conceptions") + 
  ggtitle("Estimated Number of Log Conceptions Per Month") + 
  stat_smooth(method = loess)
```
>
>Yes. There appears to be a clear discontinuity following implementation of the policy. The graph shows a jump in log estimated conceptions following the policy implementation. There is an upward trend in conceptions before the policy implementation, but the upward trend does not appear to explain the jump in estimated conceptions that take place after the policy went into effect.
>
>If there appears to be no discontinuity graphically, a researcher will likely need to revaluate their method, as their research question may not fit a RD design (it may be more in line with a DID model or something else). Or find a new cutoff point, or variable to model the discontinuity. Or, there could just be no effect regarding the cutoff they specified. Nonetheless, a lack of clear discontinuity should be a red flag for the researcher.

(e) A strategy to limit RD mistakes is to focus on observations near the cutoff. In the paper, the author restricts the data set multiple times and reports what happens to the estimate of the variable of interest $post$. Another way to check the robustness of the estimates is to change the model, whether that be adding or removing a variable.  
For this next regression, repeat what you did in part (a), but remove the cubic term $mc3$ and restrict the data set from $mc=-30$ to $mc=29$. You should have 60 observations. Again, use robust standard errors. Report on any changes in the estimates from part (a). Do your estimates (particularly for the treatment variable) appear to be robust? Create a 99\% confidence interval for $\beta_1$ (note the you cannot supply a variance-covariance matrix with the ```confint``` command).

>**Solution:**
>```{R}
subset2 = n[n$mc>-31 & n$mc<30, ]
mod2 = lm(log_n ~ factor(post) + mc + mc2 + days, data = subset2)
round(coeftest(mod2, vcov. = vcovHC(mod1, type = "HC0")), 5)
```
>The $mc$ variable is now significant. The estimate for $post$ appears to be robust as the estimate only changed slightly and is still significant at the $1\%$ significance level. Otherwise the estimates remain basically the same. 
>```{R}
# 99% Confidence Interval for Beta 1
Cov = vcovHC(mod2, "HC1")
C = 1 - 0.01
DF = mod2$df.residual
CV = abs(qt((1-C)/2, DF))
SEhat = sqrt(diag(Cov))
mod2$coefficients['factor(post)1'] + c(-1,1)*CV*SEhat['factor(post)1']
```


(f) Finally, restrict the data set from $mc=-12$ to $mc=11$. Run the same regression from part (d) with robust standard errors. What are the results, any changes from part (d)? Do our estimators seem robust as we restrict the sample size, and change the model specification?

>**Solution:**
>```{R}
subset3 = n[n$mc>-13 & n$mc<12, ]
mod3 = lm(log_n ~ factor(post) + mc + mc2 + days, data = subset3)
round(coeftest(mod3, vcov. = vcovHC(mod3, "HC0")), 5)
```
>$post$ and $mc2$ are still significant. $post$ is still significant at $5\%$ significance level, but no longer at the $1\%$ significance level. The estimates seem to be robust, as the variable of interest $post$ is significant in each regression, and the sign is what we would expect it to be each time (positive, reflecting the increase in estimated conceptions following policy implementation).

(g) Explain a potential issue with using non-parametric RD like we just did for the past two regressions? Use equation 4.5 from the book (on p.162) to help answer this question.

>**Solution:**
Non-parametric RD allows for a trade off in the bias for an increase in variance due to the decrease in number of observations. However, as you decrease the bandwidth there are not enough observations to make accurate estimates. 
$$ a_0 - b \leq a \leq a_0 + b$$
This is the equation for picking a non-parametric RD. As the bandwidth ($b$) gets smaller, the number of observations of $a$ decreases. While this can reduce bias (and non-linear trends), this also limits the number of observations that can be used for analysis around the cutoff leading to less accurate estimates.

### Question 3

(a) The regressions you just replicated are an example of a sharp RD design. Sharp RD is not the only kind of regression discontinuity design, a researcher can also use a fuzzy regression discontinuity design. From what you read in the chapter, when should researchers use a fuzzy RD as opposed to a sharp RD?

>**Solution:**
A fuzzy RD is used when the cutoff point leads to a higher intensity treatment, as opposed to a sharp RD where the cutoff differentiated between the control and the treatment. With sharp RD, the cutoff denotes access to the full treatment. In fuzzy, access to higher intensity treatment (think school examples from the book).


(b) When using a Fuzzy RD as compared to a Sharp RD, which regression would you expect to give you more significant estimates (estimates that are significantly different than zero) and why? Explain why one design will likely be superior in generating more significant estimates.

>**Solution:**
We would expect the Sharp RD to give us more significant estimates. This is because Fuzzy RD uses instrumental variables and two stage least squares regression, and that process increases the size of standard errors (as the variance of $\sigma^2$ is greater in 2SLS). Sharp RD uses straightforward OLS regression, and those standard errors will typically be smaller than any 2SLS regression standard errors.
