---
title: 'GSE 520: Problem Set #6'
author: 
  - "Rus Adamovics-Davtian"
  - "Nick Brown"
  - "Amy Ozee"
output: html_document
---

### Question 1

Many times before estimating a linear regression model the analyst may perform some transformation on the data.  For example, if one of the variables is in dollars, they may divide it by 1000, so that it is reported in units of 1,000's of dollars.  Consider $X$ the original matrix of data (including a constant) and assume that all of the assumptions of the linear regression model are satisfied, such that 
$$Y = X \beta + \pmb \varepsilon$$
Where the OLS estimate of $\beta$ is $b = (X'X)^{-1}X'Y$

Next let $\text{T}$ be any transformation matrix that is $K \times K$ and is invertible.  
Define $X^* = X \text{T}$, which produces a linear transformation on the data.  The OLS estimate of the model with the transformed data is $b^* = (X^{*'} X^{*})^{-1} X^{*'} Y$

a. Plug in the definition of $X^*$ to the equation for $b^*$ to show that $b^* = \text{T}^{-1} b$ for any transformation matrix.

**Solution:**  
\begin{align*}
X^* = & X \text{T} \\
b^* = & (X^{*'} X^{*})^{-1} X^{*'} Y \\
= & ((X \text{T})' (X \text{T}))^{-1} (X \text{T})' Y \\
= & ((X)'(\text{T})' (X)(\text{T}))^{-1} (X)'(\text{T}) Y \\
= & (X' X)^{-1}(X)'Y ((\text{T})'(\text{T}))^{-1}(\text{T}) \\
= & b (\text{T}')^{-1}(\text{T})^{-1}(\text{T}) \\
= & \text{T}^{-1} b \\
\end{align*}

b. Use the result above to show that $\widehat{Y}$ the predicted values from the original regression equals $\widehat{Y}^*$ the predicted values from the regression with the transformed data.

>**Solution:**  
\begin{align*}
\widehat{Y} = & Xb \\
\widehat{Y}^* = & X^* b^*\\
= & (X \text{T}) (\text{T}^{-1}b) \\
= & X (\text{T} \text{T}^{-1}) b \\
= & X (I) b \\
= & Xb \\
\end{align*}  
So we can conclude that $\widehat{Y}$ is equal to $\widehat{Y}^*$.

c. Finally conclude by showing that given your findings above, the two regressions have the same predicted values, they will also have the same R-squared, that is you are proving the very important and intuitive result that any linear transformation of the data does not lead to an improved fit of the outcome variable. (note this result applies to linear transformations, i.e., non-linear transformations, like include log and squared terms may be very effective in increasing R-squared.)

>**Solution:**  
\begin{align*}
R^{2} = & 1 - \frac{SSE}{SST}\\
R^2 = & 1 - \frac{\sum_{i=1}^n (y_i - \hat y_i)}{\sum_{i=1}^n (y_i - \bar y)}\\
R^2 = & 1 - \frac{Y - \widehat Y}{Y - \overline Y}\\
\end{align*}
Also from part b, $\widehat Y$ = $\widehat Y^{*}$. Therefore,
\begin{align*}
1 - \frac{Y - \widehat Y}{Y - \overline Y} = & 1 - \frac{Y - \widehat Y^{*}}{Y - \overline Y}\\
\end{align*}


### Question 2

a.  Suppose we have the linear regression $y_i =  \beta_1 + \beta_2 x_i + \varepsilon_i$ that satisfies the assumptions of the classical regression model.  Define an estimator for $\beta_2$, 
$$b_2 = (y_2 - y_1) /(x_2 - x_1)$$
Where $y_1$ is the first observation in the sample with associated $x_1$ and and $y_2$ is the second observation with associated $x_2$.  Is this estimator for $\beta_2$ unbiased? Prove it by plugging in the value for $y_i$'s and doing algebra and taking expectations.

>**Solution:**  
\begin{align*}
b_2 = & \frac{(y_2 - y_1)}{(x_2 -x_1)} \\
b_2 = & \frac{(\beta_1 + \beta_2 x_2 + \varepsilon_2) - (\beta_1 + \beta_2 x_1 + \varepsilon_1)}{(x_2 - x_1)} \\
= & \frac{\beta_2 (x_2-x_1) + (\varepsilon_2 - \varepsilon_1)}{x_2 - x_1} \\
= & \beta_2 + \frac{\varepsilon_2 - \varepsilon_1}{x_2 - x_1} \\
\end{align*}  
so $b_2|X = \frac{(\beta_1 + \beta_2 x_2 + \varepsilon_2) - (\beta_1 + \beta_2 x_1 + \varepsilon_1)}{(x_2 - x_1)}$  
Now taking expectations according to X:
\begin{align*}
E[b_2|X] = & E[\frac{(\beta_1 + \beta_2 x_2 + \varepsilon_2) - (\beta_1 + \beta_2 x_1 + \varepsilon_1)}{(x_2 - x_1)} | X] \\
= & E[\beta_2 |X] + E[\frac{\varepsilon_2 - \varepsilon_1}{x_2 - x_1} | X] \\
\end{align*}  
Since $y_i$ follows all the classical regression assumptions, we know by assumption 3 that $E[\varepsilon|x] = 0$. Using that we are left with:
\begin{align*}
E[b_2 |X] = & E[\beta_2 |X] + E[\frac{0 - 0}{x_2 - x_1} | X] \\
= & E[\beta_2 |X] \\
= & \beta_2 \\
\end{align*}  
\begin{align*}
E(b_2) = & E_X[E(b_2)|X] \\
= & E_X (\beta_2) \\
= & \beta_2 \\
\end{align*}
So $b_2$ is an unbiased estimator for $\beta_2$.

b.  Consider the multiple regression model containing three independent variables
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$$
Assume that all of the assumptions of the linear regression model are satisfied.  
You are interested in estimating the sum of the parameters on $x_1$ and $x_2$; call this $\theta = \beta_1 + \beta_2$.  Show that $\hat{\theta} = \hat{\beta}_1 + \hat{\beta}_2$ is an unbiased estimator of $\theta$ where $\hat{\beta}_1$ and $\hat{\beta}_2$ are the OLS estimates of a regression of $x_1$, $x_2$, and $x_3$ on $y$.

>**Solution:**  
To show that $\hat \theta = \hat \beta_1 + \hat \beta_2$ is unbiased, show $E[\hat \theta] - \theta = 0$. Using Law of Expectation and Law of Expected Iteration...
>\begin{align*}
E[\hat \theta] = & E[\hat \beta_1 + \hat \beta_2]\\
E[\hat \theta] = & E[\hat \beta_1] + E[\hat \beta_2]\\
E[\hat \theta] = & E_x[E[\hat \beta_1 | X]] + E_x[E[\hat \beta_2 | X]]
\end{align*}
>Need to solve for $E[\hat \beta_1 | X]$ and $E[\hat \beta_2 | X]$. Also, $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$ can be represented as $Y = X\beta + \mu$.
\begin{align*}
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}Y]\\
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}(X\beta_1 + \mu)]\\
E[\hat \beta_1 | X] = & E[(X^{'}X)^{-1}X^{'}X\beta_1 | X] + E[(X^{'}X)^{-1}X^{'}\mu | X]\\
E[\hat \beta_1 | X] = & E[\beta_1 | X] + (X^{'}X)^{-1}X^{'}E[\mu | X] \quad \quad \quad \quad \quad *E[\mu | X] = 0 \quad by \quad ASM \quad 3\\
E[\hat \beta_1 | X] = & E[\beta_1 | X]\\
E[\hat \beta_1 | X] = & \beta_1\\
\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}Y]\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}(X\beta_2 + \mu)]\\
E[\hat \beta_2 | X] = & E[(X^{'}X)^{-1}X^{'}X\beta_2 | X] + E[(X^{'}X)^{-1}X^{'}\mu | X]\\
E[\hat \beta_2 | X] = & E[\beta_2 | X] + (X^{'}X)^{-1}X^{'}E[\mu | X] \quad \quad \quad \quad \quad *E[\mu | X] = 0 \quad by \quad ASM \quad 3\\
E[\hat \beta_2 | X] = & E[\beta_2 | X]\\
E[\hat \beta_2 | X] = & \beta_2
\end{align*}
> Therefore, 
\begin{align*}
E[\hat \theta] = & E_x[E[\hat \beta_1 | X]] + E_x[E[\hat \beta_2 | X]]\\
E[\hat \theta] = & E_x[\beta_1] + E_x[\beta_2]\\
E[\hat \theta] = & \beta_1 + \beta_2\\
\\
Bias = & E[\hat \theta] - \theta = 0\\
Bias = & (\beta_1 + \beta_2) - (\beta_1 + \beta_2) = 0\\
\end{align*}
> $\hat{\theta} = \hat{\beta}_1 + \hat{\beta}_2$ is an unbiased estimator of $\theta$


### Question 3

Suppose R\&D for firms in the chemical industry is only a function of sales and satisfies the assumption of the linear regression model and can be written as
\begin{align*}
rd_i = \beta_1 + \beta_2 sales_i + \beta_3 sales^2_i + \varepsilon_i
\end{align*}
Where $E(\varepsilon | sales, sales^2) = E(\varepsilon ) = 0$

a. Consider a model without the squared term, with $rd_i = \beta_1 + \beta_2 sales_i + \varepsilon_i^*$, where $\varepsilon_i^* = \beta_3 sales^2_i + \varepsilon_i$.  Show by taking the conditional expectation of $\varepsilon_i^*$ with respect to $sales$ that this model violates the assumption of mean independence when $\beta_3 \neq 0$.

**Solution:** 
\begin{align*}
E[\varepsilon_i^*|sales] = & E[\beta_3 sales^2_i + \varepsilon_i |sales] \\
= & E[\beta_3 sales^2_i |sales] + E[\varepsilon_i |sales] \\
= & E[\beta_3 sales^2_i|sales] + 0 \\
= & \beta_3 \\
\end{align*}  
Since the conditional expectation of $\varepsilon_i^*$ with respect to sales is not $0$ and instead is equal to $\beta_3$, this model violates the assumption of mean independence (unless $\beta_3 = 0$).

b. Suppose we have the following estimates form the true model
$$\widehat{rd} = 17.79 -327.48(sales) +261.26(sales^2)$$
Let $\widetilde{b}_2$ be the estimate of the coefficient on sales when the squared term is NOT included in the regression.  Would $\widetilde{b}_2$ be larger or smaller than $-327$? Explain.

**Solution:** 
$\widetilde{b}_2$ would be larger than $-327$ in a positive direction. When $sales^2$ is included in the regression it has a positive sign and large coefficient. This coefficient will cancel out the negative coefficient of $sales$ for most values of sales (as $sales^2$ will grow faster than $sales$ as the number of sales increases). So you would expect by removing the $sales^2$ variable from the regression that the $sales$ variable will have to reflect the positive relationship between R\&D and sales. Therefore $\widetilde{b}_2$ would be larger. 


### Question 4

The dataset ```voucher.RData``` contains data on elementary school students in 1994. For this analysis, remove all student who have a "NA" for the variables $mnce90$. 

A portion of these students  where awarded school vouchers to attend a charter, private or public school of their choosing.  The remaining students attended their local public school.  The variable $mnce_i$ is a math test for student $i$ and $select_i$ equals one if the student was selected to receive a voucher and zero otherwise.  

a. What is the mean and standard deviation of the math test score variables?

**Solution:** 
```{R}
load("~/Desktop/ECON 520/Data/voucher.RData")
data = data[! is.na(data$mnce90),]

# Find the mean and standard deviation of the math test score variables
# Mean and SD of MNCE variable
mean(data$mnce)
sd(data$mnce)

# Mean and SD of MNCE90 variable
mean(data$mnce90)
sd(data$mnce90)
```

b. Standardize the test score variables by substracting the mean and dividing by the standard deviation.  Call this variable $mnceZ_i$. Estimate a simple linear regression model of the form:
$$ mnceZ_i = \beta_0 + \beta_1 select_i + \varepsilon_i$$
Report your results in equation form. According to the results do students that received a voucher do better or worse on the math test relative to those students that did not receive a voucher?  By how many standard deviations in test score units do voucher students perform better or worse?

**Solution:**  
```{R}
data$mnceZ_i = (data$mnce - mean(data$mnce)) / (sd(data$mnce))
model1 = lm(mnceZ_i ~ select, data = data)
summary(model1)
```
$$ \widehat{mnceZ_i} = 0.02 - 0.07 select_i$$
Voucher students perform worse on the math test by 0.07 standard deviations.   


c. Does your analysis in part b suggest that students that use vouchers cause math test scores to be lower?  Explain.

>**Solution:**  
The analysis appears to show that students who use vouchers cause math test scores to be lower, however that is not the case. We can only say our coefficient has a causal effect if the linear regression model assumptions are satisfied. Specifically the assumption that all important variables are specified in the regression equation, and are not nested in the error term. Looking at the $R^2$ and $adj-R^2$ we can see the $R^2$ is very low (0.001) and the $adj-R^2$ is negative (-0.001). This tells us we don't have a great fit only using the $select_i$ variable, which implies there are important explanatory variables that have not been used in the equation. So the analysis in part b did not give us a causal variable, instead a BLP (best linear predictor).

d. It is possible that vouchers are not randomly assigned to students.  Consider three sets of student variables: gender, race/ethnicity, and past academic performance.  The variable $female$ is an indicator if the student is female.  The variables $hispanic$ and $black$ are indicators for the race/ethnicity of the student.  The omitted race/ethnic group is non-black/non-hispanic.  Finally the variables $mnce90$ is a math score for the student 4 years earlier, which is a measure of past academic performance.
<br/><br/>
Run a regression with the variable $select$ as the dependent variable and the variables listed above as explanatory variables.  Report your results in equation form.  Do these results suggest the vouchers are randomly assigned or do the students that receive vouchers vary systematically from the students that do not receive the vouchers?  Use the results from this regression to characterize the main difference between students that received these vouchers versus those that did not, e.g., those likely to receive vouchers are less likely to be female, etc. 

**Solution:**  
```{R}
model2 = lm(select ~ female + hispanic + black + mnce90, data = data)
summary(model2)
```
$$ \widehat{select_i} = 0.099 + 0.033* female + 0.46 *hispanic + 0.27 *black - 0.001* mnce90$$
From the results of the regression, we would expect students who are hispanic and black to be more likely to receive vouchers than students who are not. Students who are female are slightly more likely than male students to receive a voucher, however it is a pretty small coefficient. Math test scores from 1990 seems to have very little effect on receiving a voucher. Once again, the $R^2$ and $adj-R^2$ are pretty low, so it's unknown how much stock we should put into these predictors.
  

e. Re-do the analysis in part (b) but now add control variables mentioned in part (d).  Report your results in equation form.  According to the results do students that received a voucher do better or worse on the math test relative to those students that did not receive a voucher once we control for these additional variables?  By how many standard deviations in test score units do voucher students perform better or worse?

**Solution:**  
```{R}
model3 = lm(mnceZ_i ~ select + female + hispanic + black + mnce90, data = data)
summary(model3)
```
According to the results, the students who have received a voucher perform 0.14 standard deviations better than students who have not received a voucher.


f. Which results are more likely to represent the causal effect of vouchers on test scores, the results in part (b) or the results from part (e)?  Why?

>**Solution:**  
The results in part (e) are more likely to represent the causal effect of vouchers. The model in part (e) accounts for more variables than than the model in part (b). The fit of the model in part (e) suggests it features more important explanatory variables, as the $R^2$ and $adj-R^2$ increased significantly. 


g.  Being as clear as possible explain how each of the variables, $female$, $black$, $hispanic$, and $mnce90$ contribute to the omitted variable bias when they are not included in the regression in part (b).  Which variable contributes the least, which contributes the most?

**Solution:**  
Omitted variable bias comes from the effect of an omitted variable on the dependent variable in the regression equation, multiplied by the coefficient that omitted variable has when used as an explanatory variable for X (X being a matrix of explanatory variables in the original regression equation). Applying this to specific problem, we have calculated the coefficient the omitted variables have when used as an exploratory variable for X (X being the $select$ variable) in part (d). That regression gave us all non-zero coefficients for these variable. Thus the only way these variables would not contribute to the omitted variable bias is if they had no effect on the dependent variable $mnceZ_i$. After running the regression in part (e), we can see they all had some effect on $mnceZ_i$ so we know there is omitted variable bias in the regression of part (b). The variable that contributes least to the omitted variable bias is $mnce90$ as it had the smallest coefficient ($-0.001$) from the regression we ran in part (d). The variable with the biggest contribution to omitted variable bias is $hispanic$ as it had the largest coefficient ($0.46$) from the regression in part (d).


h. Finally, assume you had access to a variable about parental income.  Explain under what scenarios the inclusion of this additional variable in the model from part (e), would make the coefficient on $select$ smaller than you found in part (e). 

**Solution:**  
A scenario under which the inclusion of an additional variable would make the coefficient on $select$ smaller would be one that $select$ is highly correlated with. If $select$ can be explained by another variable (in a regression where $select$ is the dependent variable, and the new variable has a large coefficient), then the coefficient of $select$ will be smaller than in part (e). This is due to the new variable explaining part of the math test score variables that $select$ was previousy explaining. 



### Question 5

Consider the simple linear regression model, $y_i = \beta_1 + \beta_2 x_i + \varepsilon_i$ that satisfies mean independence $E(\varepsilon | x) = E(\varepsilon) = 0$.  

Suppose $x$ is not observed.  However, $x^*$ is observed which is defined as $x_i^* = x_i + u_i$, where $u$ is called measurement error.  Assume that $E(u)=0$ and that $E(u|x)=0$.  That is, $u$ does not depend on $x$.  This is called classical measurement error because it is purely random.  Since $x$ is not observed, consider the regression of $y$ on $x^*$:
$$y_i = \beta_1 + \beta_2 x_i^* + \varepsilon^*_i$$

a.  Write explicitly the determinants of $\varepsilon^*_i$.  You do this by plugging in the $x_i = x_i^* - u_i$ into the original equation and re-arrange terms.

**Solution:**  
\begin{align*}
y_i = & \beta_1 + \beta_2(x_i^* + u_i) + \varepsilon^*_i \\
\varepsilon^*_i = & y_i - \beta_1 - \beta_2(x_i^{*} + u_i) \\
= & y_i - \beta_1 - \beta_2 x_i^* - \beta_2 u_i \\
\end{align*}


b. Show that $\varepsilon^*_i$ is not mean independent of $x_i^*$.

**Solution:**  
\begin{align*}
E[\varepsilon^*_i |x_i^*] = & E[y_i - \beta_1 - \beta_2 x_i^* - \beta_2 u_i|x_i^*] \\
= & E[y_i |x_i^*] - E[\beta_1 | x_i^*] - E[\beta_2 x_i^* | x_i^*] - E[\beta_2 u_i | x_i^*] \\
= & E[y_i |x_i^*] - E[\beta_1 | x_i^*] - E[\beta_2 x_i^* | x_i^*] - 0 \\
= & E[y_i |x_i^*] - \beta_1 - \beta_2 \\
\end{align*}


c. Treating $u$ as an omitted variable, show that if you estimate  $y_i = \beta_1 + \beta_2 x_i^* + \varepsilon^*_i$ with OLS, the estimate of $\beta_2$ will always be biased towards zero, that is, if $\beta_2>0$, the bias will be negative and if $\beta_2<0$ the bias will be positive.  This is called attenuation bias because it always produces estimated effects that are always weakened (closer to zero).

**Solution:**  



