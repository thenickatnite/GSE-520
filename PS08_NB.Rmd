---
title: 'GSE 520: Problem Set #1'
author: 
  - "Nick Brown"
  - "Nathaniel Cinnamon"
  - "Russell McIntosh"
output: html_document
---

### Question 1

The dataset ```hprice1.Rdata``` contains home prices for 88 homes as well as various housing characteristics.  We are interested in understanding the relationship between housing characteristics and home prices.  Consider the equation:
$$price = \beta_1 + \beta_2 bdrms + \beta_3 (sqrft/100) + \beta_4 (lotsize/100) + \varepsilon$$
Where $price$ is selling price in units of \$1,000's, $bdrms$ is the number of bedrooms, $sqrft$ is the number of square feet of the house, and $lotsize$ is the lot size in square feet.

The OLS estimates of the parameters and the standard errors are 
```{r,echo=FALSE}
load('../Data/hprice1.RData')
mod = lm(price~bdrms+I(sqrft/100)+I(lotsize/100),data=data)
print('Least Squares Estiamtes')
mod$coefficients
print('Standard Errors')
sqrt(diag(vcov(mod)))
```

(a) List two variables that influence home price that are captured in $\varepsilon$.

>**Solution:**  
Two variables that influence home price that are captured in $\varepsilon$ are location of the home (i.e. what city and state is the home located in) and where the home is in proximity to a school.

(b) How do you interpret the coefficient $\beta_3$?

>**Solution:**  
The coefficient $\beta_3$ represents a $\$12,000$ increase in price for every additional 100 square feet in the home.

(c) Using only matrices, in ```R``` reproduce the least squares estimates and the estimates of the standard errors using the data set.

**Solution:**  
```{R}
# Reproduce the least squares estimates using matrices
load('../Data/hprice1.RData')
data = data[! is.na(data$price),]
n = nrow(data)
X = cbind(rep(1,n), data$bdrms, (data$sqrft)/100, (data$lotsize)/100)
Y = data$price
b = solve(t(X) %*% X) %*%(t(X) %*% Y)
b

# Find the estimates of the standard errors
s = sum((Y - X %*%b)**2 / (nrow(X) - ncol(X)))
sqrt(diag(s * solve(t(X)%*%(X))))
```

(d) Next, using the ```lm``` and ```vcov``` function as I did above, re-estimate the model including the standard errors adding the variable 'colonial' to the model.  Show the results of the new model and explain ***how*** and ***why*** the parameter estimate on $bdrms$ changed with the addition of the new variable. (Recall: if you define $z$ as the new variable it is helpful to know how $z$ relates to $bdrms$, $sqrt$, and $lotsize$ by estimating $\widehat{z} = \hat{\alpha}_1  + \hat{\alpha}_2 bdrms  + \hat{\alpha}_3 (sqrft/100)  +  \hat{\alpha}_4 (lotsize/100)$.)

**Solution:**  
```{R}
mod1 = lm(price~bdrms+I(sqrft/100)+I(lotsize/100) + colonial,data=data)
print('Least Squares Estimates')
mod1$coefficients
print('Standard Errors')
sqrt(diag(vcov(mod1)))
```
The reason why the bedrooms variable was affected by the addition of the colonial varaible is due to the correlation between the two variables. We can see this by running a regression using colonial as the dependent variable and bdrms and the other explanatory variables as the explanatory variables.

```{R}
mod2 = lm(colonial ~ bdrms + I(sqrft/100)+I(lotsize/100),data = data)
print('Least Squares Estimates')
mod2$coefficients
```
Interpreting the coefficients we can see that bdrms has the largest coefficient in explaining colonial. 


(e) Finally, appealing to the formula that determines the variance of a single parameter, explain why the standard error on $bdrms$ changed when 'colonial' was added to the regression.

**Solution:**  


### Question 2

For this problem it is acceptable to use the built in ```lm``` function.

The dataset ```hprice1.Rdata``` contains home prices for 88 homes as well as various housing characteristics.  We are interested in understanding the relationship between housing characteristics and home prices.  Consider the equation:
$$price = \beta_1 + \beta_2 bdrms + \beta_3 (sqrft/100) + \beta_4 (lotsize/100) + \varepsilon$$
Where $price$ is selling price in units of \$1,000's, $bdrms$ is the number of bedrooms, $sqrft$ is the number of square feet of the house, and $lotsize$ is the lot size in square feet.

The OLS estimates using the 88 observations are
```{r,echo=FALSE}
load('../Data/hprice1.RData')
mod = lm(price~bdrms+I(sqrft/100)+I(lotsize/100),data=data)
summary(mod)
```

(a) Use these estimates to construct a 95\% confidence interval for $\beta_3$.  How did you choose your critical value?

>**Solution:**  
To create a 95\% confidence interval for $\beta_3$ we can use this basic equation:
$$[b_k - CV \times \widehat{SE(b_k)}, b_k + CV \times \widehat{SE(b_k)}]$$
From the summary above, we have $b_k = 12.278$ and $\widehat{SE(b_k)} = 1.323$ so in order to construct the confidence interval we need to determine the critical value. To get the critical value we need to look at a t distribution with $(n-K)$ degrees of freedom. From a t-table with $88-4$ degrees of freedom the critical value would be $1.990$.
So a 95\% confidence interval would look like:
$$[12.278 - 1.990 \times 1.323, 12.278 + 1.990 \times 1.323]$$
So for 95\% of samples, the true value of $\beta_3$ will be between $[9.645, 14.911]$. 


(b) Next, consider a house that has $bdrms=3$, $sqrft=2000$, and $lotsize=6000$.  We can use the estimates from our model to predict the selling price for this house.  Let $x_o = [\begin{matrix} 1 & 3 & 20 & 60 \end{matrix} ]'$ be the vector of characteristics for this house.  The predicted selling price is $\hat{y}_o = x_o'b$.  What is the predicted selling price for this house?

>**Solution:**  
>```{R}
x_0 = matrix(c(1,2,20,60), ncol=1)
t(x_0)%*%b
```
The predicted selling price for this home is $\$263,897.30$.


(c) $\hat{y}_o$ is an estimate of $E(y | x_o)$. As an estimate, $\hat{y}_o$ itself is a random variable, i.e., with a different sample we would have a different value.  We can use our OLS estimates to construct a 95\% confidence interval for $E(y | x_o)$ by using the fact that
$$\frac{\hat{y}_o - E(y | x_o)}{\widehat{SE(\hat{y}_o)}} \sim t[n-K]$$
To construct the confidence interval we need $\widehat{SE(\hat{y}_o)}$.  Use the fact that $\hat{y}_o = x_o'b$ to show that $\widehat{Var(\hat{y}_o)} = x_o' \widehat{Var(b)}x_o$.
Construct a 95\% confidence interval for $E(y | x_o)$ using the command  
    ```
x_o = matrix(c(1,3,20,60))
yhat_o = t(x_o) %*% mod$coefficients
SEyhat_o = sqrt(t(x_o) %*% vcov(mod) %*% x_o)
```
    Choose the appropriate critical value to construct the confidence interval.  THIS IS CALLED A CONFIDENCE INTERVAL FOR PREDICTED VALUES.

**Solution:**  


(d) We can use our estimates to construct something called a PREDICTION INTERVAL.  While $\hat{y}_o$ is our best guess at the selling price, the actual selling price will be determined by $y_o = x_o' \beta + \varepsilon_o$.  A prediction interval gives a range of values such that there is a 95\% chance the random interval will contain the actual selling price.  

    Consider the random variable actual selling price minus predicted selling price, $y_o - \hat{y}_o= x_o' \beta + \varepsilon_o - x_o'b$.  What is $E(y_o - \hat{y}_o)$?  Show that $Var(y_o - \hat{y}_o) = x_o' \widehat{Var(b)}x_o + \sigma^2$

**Solution:**  

(e) Construct a 95\% prediction interval for $y_o$ using the fact that 
$$\frac{y_o- \hat{y}_o}{\sqrt{x_o' \widehat{Var(b)}x_o + s^2}} \sim t[n-K]$$
You can use the following code to help construct the prediction interval.
    ```
s2 = sum(mod$residuals^2)/mod$df.residual
vy_o = t(x_o) %*% vcov(mod) %*% x_o + s2
```

**Solution:**  


(f) Finally, run the following commands in ```R```.  Explain what each of these commands is doing relative to what you just did by hand above.
    ```
mod = lm(price~bdrms+I(sqrft/100)+I(lotsize/100),data=data)
confint(mod)
pdata = data.frame(bdrms=3,sqrft=20*100,lotsize=60*100)
predict(mod,pdata,interval="prediction")
predict(mod,pdata,interval="confidence")
```

**Solution:**  


### Question 3

In ``Parental Education and Offspring Outcomes: Evidence from the Swedish Compulsory Schooling Reform'', Petter Lundborg, Anton Nilsson, and Dan-Olof Rooth set out to study the impact of parental educational attainment on the development of cognitive and noncognitive skills in their children (cognitive skills reflect hard skills, for example how good someone is at logic and reasoning.  Noncognitive skills reflect soft skills, for example the motivation and self-discipline of an individual).  Letting $H^c$ denote a given measurement for the outcome of the child, the researchers where interested in estimating the econometric equation:
\begin{align}
H^c = \alpha_0 + \alpha_1 S^p + \varepsilon
\end{align}
Where $S^p$ represents the years of education of either the father or the mother (they estimate two separate equations, one where the mother's education level effects the outcome and one where the father's education effects the outcome).  The structural error $\varepsilon$ includes all of the other unobserved components effecting the outcome.

The authors offer a causal and non-causal explanation for any possible empirical relationship between parental education and the outcomes of their children:

"Commonly discussed causal pathways have been the improved knowledge and the greater economic resources that follow with greater education. The latter pathway refers to the fact that higher education usually also means higher income."  
  
"In particular, since parents and children share common genes, any relationship between them may be generated through unobserved genetic endowments."

(a) The authors begin by estimating the equation above using ordinary least squares.  Which of the assumptions in the classic linear regression model would be most important in order to view the estimate of $\alpha_1$ as a causal effect of parental education on the outcome $H^c$?

**Solution:**  


(b)
 Suppose all of the assumptions of the classic linear regression model are satisfied.  The results of these estimates are found in the table below (standard errors are in parenthesis):

    Variable	 | Cognitive Ability | Noncognitive Ability
-------  |-----  | -----------------
Mother Years of Schooling ($\alpha_1$) | 0.1188 (0.0009) | 0.0651 (0.0008)
Father Years of Schooling ($\alpha_1$) | 0.1090 (0.0009) | 0.0583 (0.0008)

    In one sentence, summarize *qualitatively* the results of the coefficient estimates in this table.  In one sentence provide *one quantitative* result expressed in this table (the dependent variables have been standardized to have mean zero and standard deviation of one).

**Solution:**  


(c) The question does not state the sample size.  Given their extremely simple model they likely have a very small R-squared.  However, they also have extremely small standard errors on the parameters.  What do these two facts suggest about the size of the sample used in estimation. Explain.

**Solution:**  


(d) Using these results, construct a 95\% confidence interval for the estimate for the impact of an additional year of education for the father on noncognitive skills of their children.

**Solution:**  

  

### Question 4

For each statement below determine whether the statement is true or false and explain your answer

(a) In the population, the distribution of $Y$ conditional on $X$ can be written as $P(Y | X)$.  Suppose this distribution satisfies the set of assumptions in the classical linear regression model.  If we estimate $\beta$ from two different random samples from the population, then we will get two different answers because $\beta$ is an unknown random variable in the population. 

**Solution:**  


(b)  Consider the long regression $y = x_1 \beta_1 + x_2 \beta_2 + \varepsilon$, then the estimate of the short regression $y = x_1 \beta_1 + \widetilde{\varepsilon}$ is always biased if $E(x_2 | x_1) \neq E(x_2)$.

**Solution:**  


(c) In the classical linear regression model we assumed normal disturbance, i.e. $\varepsilon \sim \mathcal{N}(0,\sigma^2)$.  Because of this assumption we are able to show that the variance of the least squares estimator was $Var(b | X) = \sigma^2 (X'X)^{-1}$

**Solution:**  


### Question 5

Discuss which (if any) assumptions of the linear regression model are violated from each the following and whether they cause least squares to be biased.

(a) Heteroskedasticity. 

**Solution:**  


(b) Omitting an important variable. 

**Solution:**  


(c)  A sample correlation coefficient of .95 between two independent variables both included in the model.

**Solution:**  


